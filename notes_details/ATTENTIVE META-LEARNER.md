ABSTRACT
Deep neural networks excel in regimes with large amounts of data, but tend to
struggle when data is scarce or when they need to adapt quickly to changes in the
task. In response, recent work in meta-learning proposes training a meta-learner
on a distribution of similar tasks, in the hopes of generalization to novel but related
tasks by learning a high-level strategy that captures the essence of the problem it is
asked to solve. We propose a class of simple and generic meta-learner architectures that
use a novel combination of temporal convolutions and soft attention; the former to
aggregate information from past experience and the latter to pinpoint specific pieces
of information. 

![attentive meta learner](https://github.com/fandulu/paper-notes)
